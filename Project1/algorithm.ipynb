{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import NuSVR, LinearSVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2090, 9)\n",
      "(2090,)\n"
     ]
    }
   ],
   "source": [
    "train_data_path = './dataset/train_data.csv'\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data.drop(columns=['Name'], inplace=True)\n",
    "X = train_data.to_numpy(dtype='float64')\n",
    "X.shape\n",
    "y = X[:, -1]\n",
    "X = X[:, :-1]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.044309591930424955\n"
     ]
    }
   ],
   "source": [
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "prediction = LR.predict(X_test)\n",
    "error = prediction - y_test\n",
    "MSE = np.sum(np.power(error, 2)) / error.shape[0]\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 41184.00155029\n",
      "Iteration 2, loss = 40797.14245590\n",
      "Iteration 3, loss = 40413.04776730\n",
      "Iteration 4, loss = 40031.77036351\n",
      "Iteration 5, loss = 39653.31003730\n",
      "Iteration 6, loss = 39277.61576489\n",
      "Iteration 7, loss = 38904.66515443\n",
      "Iteration 8, loss = 38534.46489620\n",
      "Iteration 9, loss = 38167.00282942\n",
      "Iteration 10, loss = 37802.15151870\n",
      "Iteration 11, loss = 37440.04366004\n",
      "Iteration 12, loss = 37080.55685618\n",
      "Iteration 13, loss = 36723.74674167\n",
      "Iteration 14, loss = 36369.49797705\n",
      "Iteration 15, loss = 36017.87970402\n",
      "Iteration 16, loss = 35668.86368285\n",
      "Iteration 17, loss = 35322.35383388\n",
      "Iteration 18, loss = 34978.35788876\n",
      "Iteration 19, loss = 34636.85943058\n",
      "Iteration 20, loss = 34297.79438108\n",
      "Iteration 21, loss = 33961.16827942\n",
      "Iteration 22, loss = 33626.92006447\n",
      "Iteration 23, loss = 33295.16571092\n",
      "Iteration 24, loss = 32965.79454685\n",
      "Iteration 25, loss = 32638.76825780\n",
      "Iteration 26, loss = 32314.08028447\n",
      "Iteration 27, loss = 31991.74194621\n",
      "Iteration 28, loss = 31671.63417036\n",
      "Iteration 29, loss = 31353.82178137\n",
      "Iteration 30, loss = 31038.26600240\n",
      "Iteration 31, loss = 30724.95137517\n",
      "Iteration 32, loss = 30413.85025312\n",
      "Iteration 33, loss = 30104.94476691\n",
      "Iteration 34, loss = 29798.22067402\n",
      "Iteration 35, loss = 29493.69085475\n",
      "Iteration 36, loss = 29191.28488830\n",
      "Iteration 37, loss = 28890.97037635\n",
      "Iteration 38, loss = 28592.82876573\n",
      "Iteration 39, loss = 28296.74150289\n",
      "Iteration 40, loss = 28002.76618183\n",
      "Iteration 41, loss = 27710.85717351\n",
      "Iteration 42, loss = 27421.00408377\n",
      "Iteration 43, loss = 27133.16214611\n",
      "Iteration 44, loss = 26847.38541310\n",
      "Iteration 45, loss = 26563.54428862\n",
      "Iteration 46, loss = 26281.75391889\n",
      "Iteration 47, loss = 26001.91063997\n",
      "Iteration 48, loss = 25724.05095515\n",
      "Iteration 49, loss = 25448.11073496\n",
      "Iteration 50, loss = 25174.08726217\n",
      "Iteration 51, loss = 24902.05098621\n",
      "Iteration 52, loss = 24631.85144874\n",
      "Iteration 53, loss = 24363.63903513\n",
      "Iteration 54, loss = 24097.26638632\n",
      "Iteration 55, loss = 23832.78276274\n",
      "Iteration 56, loss = 23570.20152378\n",
      "Iteration 57, loss = 23309.45396548\n",
      "Iteration 58, loss = 23050.56596077\n",
      "Iteration 59, loss = 22793.52659014\n",
      "Iteration 60, loss = 22538.29916891\n",
      "Iteration 61, loss = 22284.91519625\n",
      "Iteration 62, loss = 22033.34027155\n",
      "Iteration 63, loss = 21783.56711619\n",
      "Iteration 64, loss = 21535.60217060\n",
      "Iteration 65, loss = 21289.41262178\n",
      "Iteration 66, loss = 21044.99543418\n",
      "Iteration 67, loss = 20802.34555239\n",
      "Iteration 68, loss = 20561.47786014\n",
      "Iteration 69, loss = 20322.33577151\n",
      "Iteration 70, loss = 20084.95390493\n",
      "Iteration 71, loss = 19849.29589364\n",
      "Iteration 72, loss = 19615.36100662\n",
      "Iteration 73, loss = 19383.14166201\n",
      "Iteration 74, loss = 19152.61415442\n",
      "Iteration 75, loss = 18923.81055027\n",
      "Iteration 76, loss = 18696.69872727\n",
      "Iteration 77, loss = 18471.28446637\n",
      "Iteration 78, loss = 18247.54006283\n",
      "Iteration 79, loss = 18025.46409487\n",
      "Iteration 80, loss = 17805.04839496\n",
      "Iteration 81, loss = 17586.28963041\n",
      "Iteration 82, loss = 17369.19019395\n",
      "Iteration 83, loss = 17153.72062165\n",
      "Iteration 84, loss = 16939.89744513\n",
      "Iteration 85, loss = 16727.70019356\n",
      "Iteration 86, loss = 16517.13892691\n",
      "Iteration 87, loss = 16308.18046686\n",
      "Iteration 88, loss = 16100.84142762\n",
      "Iteration 89, loss = 15895.12816516\n",
      "Iteration 90, loss = 15690.99765724\n",
      "Iteration 91, loss = 15488.48843165\n",
      "Iteration 92, loss = 15287.57752237\n",
      "Iteration 93, loss = 15088.22347421\n",
      "Iteration 94, loss = 14890.46299309\n",
      "Iteration 95, loss = 14694.26459096\n",
      "Iteration 96, loss = 14499.65595767\n",
      "Iteration 97, loss = 14306.58843797\n",
      "Iteration 98, loss = 14115.09237824\n",
      "Iteration 99, loss = 13925.12715294\n",
      "Iteration 100, loss = 13736.72301356\n",
      "Iteration 101, loss = 13549.86610847\n",
      "Iteration 102, loss = 13364.51283722\n",
      "Iteration 103, loss = 13180.73198273\n",
      "Iteration 104, loss = 12998.46994443\n",
      "Iteration 105, loss = 12817.72649461\n",
      "Iteration 106, loss = 12638.51352449\n",
      "Iteration 107, loss = 12460.82213582\n",
      "Iteration 108, loss = 12284.62032507\n",
      "Iteration 109, loss = 12109.95485172\n",
      "Iteration 110, loss = 11936.75332660\n",
      "Iteration 111, loss = 11765.07061093\n",
      "Iteration 112, loss = 11594.85016766\n",
      "Iteration 113, loss = 11426.14117798\n",
      "Iteration 114, loss = 11258.90736584\n",
      "Iteration 115, loss = 11093.14260071\n",
      "Iteration 116, loss = 10928.85758100\n",
      "Iteration 117, loss = 10766.04077446\n",
      "Iteration 118, loss = 10604.67415609\n",
      "Iteration 119, loss = 10444.78972827\n",
      "Iteration 120, loss = 10286.33992647\n",
      "Iteration 121, loss = 10129.34785274\n",
      "Iteration 122, loss = 9973.79553344\n",
      "Iteration 123, loss = 9819.67827266\n",
      "Iteration 124, loss = 9667.00030950\n",
      "Iteration 125, loss = 9515.75486268\n",
      "Iteration 126, loss = 9365.93010520\n",
      "Iteration 127, loss = 9217.52603184\n",
      "Iteration 128, loss = 9070.54016270\n",
      "Iteration 129, loss = 8924.96122556\n",
      "Iteration 130, loss = 8780.81131114\n",
      "Iteration 131, loss = 8638.03225119\n",
      "Iteration 132, loss = 8496.68756659\n",
      "Iteration 133, loss = 8356.72391182\n",
      "Iteration 134, loss = 8218.16015056\n",
      "Iteration 135, loss = 8080.98958118\n",
      "Iteration 136, loss = 7945.19879320\n",
      "Iteration 137, loss = 7810.79829789\n",
      "Iteration 138, loss = 7677.78248953\n",
      "Iteration 139, loss = 7546.12980367\n",
      "Iteration 140, loss = 7415.84771377\n",
      "Iteration 141, loss = 7286.91834838\n",
      "Iteration 142, loss = 7159.37232212\n",
      "Iteration 143, loss = 7033.16185371\n",
      "Iteration 144, loss = 6908.31516777\n",
      "Iteration 145, loss = 6784.83029043\n",
      "Iteration 146, loss = 6662.65540128\n",
      "Iteration 147, loss = 6541.83894211\n",
      "Iteration 148, loss = 6422.35308176\n",
      "Iteration 149, loss = 6304.19860346\n",
      "Iteration 150, loss = 6187.37931226\n",
      "Iteration 151, loss = 6071.85366283\n",
      "Iteration 152, loss = 5957.66416908\n",
      "Iteration 153, loss = 5844.79915662\n",
      "Iteration 154, loss = 5733.21869759\n",
      "Iteration 155, loss = 5622.96764639\n",
      "Iteration 156, loss = 5513.99622135\n",
      "Iteration 157, loss = 5406.33358948\n",
      "Iteration 158, loss = 5299.95316381\n",
      "Iteration 159, loss = 5194.87078081\n",
      "Iteration 160, loss = 5091.06641494\n",
      "Iteration 161, loss = 4988.53080759\n",
      "Iteration 162, loss = 4887.26887356\n",
      "Iteration 163, loss = 4787.27370825\n",
      "Iteration 164, loss = 4688.55443692\n",
      "Iteration 165, loss = 4591.09149417\n",
      "Iteration 166, loss = 4494.88060382\n",
      "Iteration 167, loss = 4399.92091563\n",
      "Iteration 168, loss = 4306.21689560\n",
      "Iteration 169, loss = 4213.74238665\n",
      "Iteration 170, loss = 4122.51267211\n",
      "Iteration 171, loss = 4032.51674254\n",
      "Iteration 172, loss = 3943.73379082\n",
      "Iteration 173, loss = 3856.17946402\n",
      "Iteration 174, loss = 3769.85095219\n",
      "Iteration 175, loss = 3684.70837485\n",
      "Iteration 176, loss = 3600.78224399\n",
      "Iteration 177, loss = 3518.07502152\n",
      "Iteration 178, loss = 3436.54759751\n",
      "Iteration 179, loss = 3356.20782551\n",
      "Iteration 180, loss = 3277.05600302\n",
      "Iteration 181, loss = 3199.06921482\n",
      "Iteration 182, loss = 3122.26250065\n",
      "Iteration 183, loss = 3046.61970131\n",
      "Iteration 184, loss = 2972.12166756\n",
      "Iteration 185, loss = 2898.79567690\n",
      "Iteration 186, loss = 2826.60695801\n",
      "Iteration 187, loss = 2755.57049605\n",
      "Iteration 188, loss = 2685.67645521\n",
      "Iteration 189, loss = 2616.88563213\n",
      "Iteration 190, loss = 2549.25008155\n",
      "Iteration 191, loss = 2482.70707520\n",
      "Iteration 192, loss = 2417.29039466\n",
      "Iteration 193, loss = 2352.97023656\n",
      "Iteration 194, loss = 2289.74892339\n",
      "Iteration 195, loss = 2227.63593768\n",
      "Iteration 196, loss = 2166.58368005\n",
      "Iteration 197, loss = 2106.63199038\n",
      "Iteration 198, loss = 2047.74366747\n",
      "Iteration 199, loss = 1989.91540793\n",
      "Iteration 200, loss = 1933.13718415\n",
      "Iteration 201, loss = 1877.41714140\n",
      "Iteration 202, loss = 1822.73576723\n",
      "Iteration 203, loss = 1769.09011071\n",
      "Iteration 204, loss = 1716.49017808\n",
      "Iteration 205, loss = 1664.89987217\n",
      "Iteration 206, loss = 1614.32584860\n",
      "Iteration 207, loss = 1564.76232858\n",
      "Iteration 208, loss = 1516.18411482\n",
      "Iteration 209, loss = 1468.62005898\n",
      "Iteration 210, loss = 1422.03342504\n",
      "Iteration 211, loss = 1376.42156495\n",
      "Iteration 212, loss = 1331.79160476\n",
      "Iteration 213, loss = 1288.12246591\n",
      "Iteration 214, loss = 1245.41970791\n",
      "Iteration 215, loss = 1203.66591159\n",
      "Iteration 216, loss = 1162.86626846\n",
      "Iteration 217, loss = 1123.01019431\n",
      "Iteration 218, loss = 1084.07103785\n",
      "Iteration 219, loss = 1046.06296921\n",
      "Iteration 220, loss = 1008.95771576\n",
      "Iteration 221, loss = 972.76394568\n",
      "Iteration 222, loss = 937.45393062\n",
      "Iteration 223, loss = 903.02201048\n",
      "Iteration 224, loss = 869.47961690\n",
      "Iteration 225, loss = 836.77649013\n",
      "Iteration 226, loss = 804.95863042\n",
      "Iteration 227, loss = 773.96181451\n",
      "Iteration 228, loss = 743.80010516\n",
      "Iteration 229, loss = 714.47320112\n",
      "Iteration 230, loss = 685.94870843\n",
      "Iteration 231, loss = 658.23358655\n",
      "Iteration 232, loss = 631.29496446\n",
      "Iteration 233, loss = 605.15698783\n",
      "Iteration 234, loss = 579.76955644\n",
      "Iteration 235, loss = 555.16712184\n",
      "Iteration 236, loss = 531.27180520\n",
      "Iteration 237, loss = 508.15149234\n",
      "Iteration 238, loss = 485.73971623\n",
      "Iteration 239, loss = 464.05347126\n",
      "Iteration 240, loss = 443.05441055\n",
      "Iteration 241, loss = 422.76004056\n",
      "Iteration 242, loss = 403.13512615\n",
      "Iteration 243, loss = 384.19111929\n",
      "Iteration 244, loss = 365.88451775\n",
      "Iteration 245, loss = 348.24026688\n",
      "Iteration 246, loss = 331.22249163\n",
      "Iteration 247, loss = 314.82543826\n",
      "Iteration 248, loss = 299.02543050\n",
      "Iteration 249, loss = 283.84072053\n",
      "Iteration 250, loss = 269.20885427\n",
      "Iteration 251, loss = 255.17328062\n",
      "Iteration 252, loss = 241.68193354\n",
      "Iteration 253, loss = 228.73393269\n",
      "Iteration 254, loss = 216.33441289\n",
      "Iteration 255, loss = 204.44002245\n",
      "Iteration 256, loss = 193.05427073\n",
      "Iteration 257, loss = 182.17535136\n",
      "Iteration 258, loss = 171.74892885\n",
      "Iteration 259, loss = 161.81855080\n",
      "Iteration 260, loss = 152.33553839\n",
      "Iteration 261, loss = 143.29276183\n",
      "Iteration 262, loss = 134.69443469\n",
      "Iteration 263, loss = 126.49869011\n",
      "Iteration 264, loss = 118.70729729\n",
      "Iteration 265, loss = 111.32014085\n",
      "Iteration 266, loss = 104.30169776\n",
      "Iteration 267, loss = 97.65601557\n",
      "Iteration 268, loss = 91.36722017\n",
      "Iteration 269, loss = 85.41664436\n",
      "Iteration 270, loss = 79.79766102\n",
      "Iteration 271, loss = 74.50490166\n",
      "Iteration 272, loss = 69.49798492\n",
      "Iteration 273, loss = 64.80487202\n",
      "Iteration 274, loss = 60.39324986\n",
      "Iteration 275, loss = 56.24086066\n",
      "Iteration 276, loss = 52.36461051\n",
      "Iteration 277, loss = 48.71969354\n",
      "Iteration 278, loss = 45.32244945\n",
      "Iteration 279, loss = 42.13981144\n",
      "Iteration 280, loss = 39.17905679\n",
      "Iteration 281, loss = 36.41754038\n",
      "Iteration 282, loss = 33.85329600\n",
      "Iteration 283, loss = 31.46806841\n",
      "Iteration 284, loss = 29.26769502\n",
      "Iteration 285, loss = 27.22830921\n",
      "Iteration 286, loss = 25.33532074\n",
      "Iteration 287, loss = 23.59248248\n",
      "Iteration 288, loss = 21.98767214\n",
      "Iteration 289, loss = 20.51674023\n",
      "Iteration 290, loss = 19.16306383\n",
      "Iteration 291, loss = 17.92314052\n",
      "Iteration 292, loss = 16.78937291\n",
      "Iteration 293, loss = 15.75158371\n",
      "Iteration 294, loss = 14.81235231\n",
      "Iteration 295, loss = 13.95030661\n",
      "Iteration 296, loss = 13.16976228\n",
      "Iteration 297, loss = 12.46580538\n",
      "Iteration 298, loss = 11.82425142\n",
      "Iteration 299, loss = 11.24440987\n",
      "Iteration 300, loss = 10.72382562\n",
      "Iteration 301, loss = 10.25429701\n",
      "Iteration 302, loss = 9.82525975\n",
      "Iteration 303, loss = 9.44592591\n",
      "Iteration 304, loss = 9.10405450\n",
      "Iteration 305, loss = 8.79709109\n",
      "Iteration 306, loss = 8.52407362\n",
      "Iteration 307, loss = 8.27535004\n",
      "Iteration 308, loss = 8.05605674\n",
      "Iteration 309, loss = 7.85654555\n",
      "Iteration 310, loss = 7.67962110\n",
      "Iteration 311, loss = 7.51827098\n",
      "Iteration 312, loss = 7.36954925\n",
      "Iteration 313, loss = 7.23702962\n",
      "Iteration 314, loss = 7.11508162\n",
      "Iteration 315, loss = 7.00211525\n",
      "Iteration 316, loss = 6.89679347\n",
      "Iteration 317, loss = 6.79956046\n",
      "Iteration 318, loss = 6.70895443\n",
      "Iteration 319, loss = 6.62181980\n",
      "Iteration 320, loss = 6.54245176\n",
      "Iteration 321, loss = 6.46954547\n",
      "Iteration 322, loss = 6.39900018\n",
      "Iteration 323, loss = 6.33447388\n",
      "Iteration 324, loss = 6.27289091\n",
      "Iteration 325, loss = 6.21458997\n",
      "Iteration 326, loss = 6.15876370\n",
      "Iteration 327, loss = 6.10545333\n",
      "Iteration 328, loss = 6.05430532\n",
      "Iteration 329, loss = 6.00478170\n",
      "Iteration 330, loss = 5.95639362\n",
      "Iteration 331, loss = 5.90933277\n",
      "Iteration 332, loss = 5.86321561\n",
      "Iteration 333, loss = 5.81816922\n",
      "Iteration 334, loss = 5.77360794\n",
      "Iteration 335, loss = 5.72977069\n",
      "Iteration 336, loss = 5.68649867\n",
      "Iteration 337, loss = 5.64445388\n",
      "Iteration 338, loss = 5.60186606\n",
      "Iteration 339, loss = 5.56020536\n",
      "Iteration 340, loss = 5.51848339\n",
      "Iteration 341, loss = 5.47751940\n",
      "Iteration 342, loss = 5.43683530\n",
      "Iteration 343, loss = 5.39548230\n",
      "Iteration 344, loss = 5.35494401\n",
      "Iteration 345, loss = 5.31399026\n",
      "Iteration 346, loss = 5.27369114\n",
      "Iteration 347, loss = 5.23426554\n",
      "Iteration 348, loss = 5.19336460\n",
      "Iteration 349, loss = 5.15419397\n",
      "Iteration 350, loss = 5.11406010\n",
      "Iteration 351, loss = 5.07470830\n",
      "Iteration 352, loss = 5.03494253\n",
      "Iteration 353, loss = 4.99597460\n",
      "Iteration 354, loss = 4.95638647\n",
      "Iteration 355, loss = 4.91707643\n",
      "Iteration 356, loss = 4.87930789\n",
      "Iteration 357, loss = 4.83939178\n",
      "Iteration 358, loss = 4.80074452\n",
      "Iteration 359, loss = 4.76220770\n",
      "Iteration 360, loss = 4.72474039\n",
      "Iteration 361, loss = 4.68606688\n",
      "Iteration 362, loss = 4.64831561\n",
      "Iteration 363, loss = 4.61054752\n",
      "Iteration 364, loss = 4.57251219\n",
      "Iteration 365, loss = 4.53414603\n",
      "Iteration 366, loss = 4.49606107\n",
      "Iteration 367, loss = 4.45938914\n",
      "Iteration 368, loss = 4.42080726\n",
      "Iteration 369, loss = 4.38273494\n",
      "Iteration 370, loss = 4.34515101\n",
      "Iteration 371, loss = 4.30740104\n",
      "Iteration 372, loss = 4.26967544\n",
      "Iteration 373, loss = 4.23283938\n",
      "Iteration 374, loss = 4.19547288\n",
      "Iteration 375, loss = 4.15867466\n",
      "Iteration 376, loss = 4.12173498\n",
      "Iteration 377, loss = 4.08457500\n",
      "Iteration 378, loss = 4.04748589\n",
      "Iteration 379, loss = 4.01166901\n",
      "Iteration 380, loss = 3.97435039\n",
      "Iteration 381, loss = 3.93871655\n",
      "Iteration 382, loss = 3.90206683\n",
      "Iteration 383, loss = 3.86560190\n",
      "Iteration 384, loss = 3.82969391\n",
      "Iteration 385, loss = 3.79460512\n",
      "Iteration 386, loss = 3.75841826\n",
      "Iteration 387, loss = 3.72382127\n",
      "Iteration 388, loss = 3.68827796\n",
      "Iteration 389, loss = 3.65285580\n",
      "Iteration 390, loss = 3.61773784\n",
      "Iteration 391, loss = 3.58344002\n",
      "Iteration 392, loss = 3.54819818\n",
      "Iteration 393, loss = 3.51375617\n",
      "Iteration 394, loss = 3.47895499\n",
      "Iteration 395, loss = 3.44460274\n",
      "Iteration 396, loss = 3.41014018\n",
      "Iteration 397, loss = 3.37599200\n",
      "Iteration 398, loss = 3.34210352\n",
      "Iteration 399, loss = 3.30856218\n",
      "Iteration 400, loss = 3.27488464\n",
      "Iteration 401, loss = 3.24115577\n",
      "Iteration 402, loss = 3.20863558\n",
      "Iteration 403, loss = 3.17538182\n",
      "Iteration 404, loss = 3.14277703\n",
      "Iteration 405, loss = 3.10986730\n",
      "Iteration 406, loss = 3.07939735\n",
      "Iteration 407, loss = 3.04475264\n",
      "Iteration 408, loss = 3.01292109\n",
      "Iteration 409, loss = 2.98057894\n",
      "Iteration 410, loss = 2.94971430\n",
      "Iteration 411, loss = 2.91846877\n",
      "Iteration 412, loss = 2.88678258\n",
      "Iteration 413, loss = 2.85701795\n",
      "Iteration 414, loss = 2.82567732\n",
      "Iteration 415, loss = 2.79503730\n",
      "Iteration 416, loss = 2.76471480\n",
      "Iteration 417, loss = 2.73490292\n",
      "Iteration 418, loss = 2.70483720\n",
      "Iteration 419, loss = 2.67479654\n",
      "Iteration 420, loss = 2.64620770\n",
      "Iteration 421, loss = 2.61663961\n",
      "Iteration 422, loss = 2.58815841\n",
      "Iteration 423, loss = 2.56082531\n",
      "Iteration 424, loss = 2.53054047\n",
      "Iteration 425, loss = 2.50330569\n",
      "Iteration 426, loss = 2.47611778\n",
      "Iteration 427, loss = 2.44918703\n",
      "Iteration 428, loss = 2.42143269\n",
      "Iteration 429, loss = 2.39480615\n",
      "Iteration 430, loss = 2.36879688\n",
      "Iteration 431, loss = 2.34211874\n",
      "Iteration 432, loss = 2.31644033\n",
      "Iteration 433, loss = 2.29068455\n",
      "Iteration 434, loss = 2.26468736\n",
      "Iteration 435, loss = 2.23974013\n",
      "Iteration 436, loss = 2.21487928\n",
      "Iteration 437, loss = 2.19021330\n",
      "Iteration 438, loss = 2.16593160\n",
      "Iteration 439, loss = 2.14187390\n",
      "Iteration 440, loss = 2.11776996\n",
      "Iteration 441, loss = 2.09432287\n",
      "Iteration 442, loss = 2.07107151\n",
      "Iteration 443, loss = 2.04821423\n",
      "Iteration 444, loss = 2.02634490\n",
      "Iteration 445, loss = 2.00275526\n",
      "Iteration 446, loss = 1.98152375\n",
      "Iteration 447, loss = 1.95937912\n",
      "Iteration 448, loss = 1.93853771\n",
      "Iteration 449, loss = 1.91672682\n",
      "Iteration 450, loss = 1.89609499\n",
      "Iteration 451, loss = 1.87562729\n",
      "Iteration 452, loss = 1.85516334\n",
      "Iteration 453, loss = 1.83535802\n",
      "Iteration 454, loss = 1.81560631\n",
      "Iteration 455, loss = 1.79514985\n",
      "Iteration 456, loss = 1.77708131\n",
      "Iteration 457, loss = 1.75712578\n",
      "Iteration 458, loss = 1.73887353\n",
      "Iteration 459, loss = 1.72037588\n",
      "Iteration 460, loss = 1.70111822\n",
      "Iteration 461, loss = 1.68354346\n",
      "Iteration 462, loss = 1.66583700\n",
      "Iteration 463, loss = 1.64816644\n",
      "Iteration 464, loss = 1.63054886\n",
      "Iteration 465, loss = 1.61403228\n",
      "Iteration 466, loss = 1.59623092\n",
      "Iteration 467, loss = 1.57963013\n",
      "Iteration 468, loss = 1.56276035\n",
      "Iteration 469, loss = 1.54709753\n",
      "Iteration 470, loss = 1.53103534\n",
      "Iteration 471, loss = 1.51412088\n",
      "Iteration 472, loss = 1.49919958\n",
      "Iteration 473, loss = 1.48307592\n",
      "Iteration 474, loss = 1.46862825\n",
      "Iteration 475, loss = 1.45244622\n",
      "Iteration 476, loss = 1.43816319\n",
      "Iteration 477, loss = 1.42327667\n",
      "Iteration 478, loss = 1.40833001\n",
      "Iteration 479, loss = 1.39414371\n",
      "Iteration 480, loss = 1.38078709\n",
      "Iteration 481, loss = 1.36679402\n",
      "Iteration 482, loss = 1.35267178\n",
      "Iteration 483, loss = 1.33926115\n",
      "Iteration 484, loss = 1.32557821\n",
      "Iteration 485, loss = 1.31238011\n",
      "Iteration 486, loss = 1.29942380\n",
      "Iteration 487, loss = 1.28712828\n",
      "Iteration 488, loss = 1.27349185\n",
      "Iteration 489, loss = 1.26037966\n",
      "Iteration 490, loss = 1.24799867\n",
      "Iteration 491, loss = 1.23517463\n",
      "Iteration 492, loss = 1.22418743\n",
      "Iteration 493, loss = 1.21127927\n",
      "Iteration 494, loss = 1.20069891\n",
      "Iteration 495, loss = 1.18782374\n",
      "Iteration 496, loss = 1.17620893\n",
      "Iteration 497, loss = 1.16485056\n",
      "Iteration 498, loss = 1.15309015\n",
      "Iteration 499, loss = 1.14192817\n",
      "Iteration 500, loss = 1.13093428\n",
      "Iteration 501, loss = 1.11956910\n",
      "Iteration 502, loss = 1.10888837\n",
      "Iteration 503, loss = 1.09848352\n",
      "Iteration 504, loss = 1.08897438\n",
      "Iteration 505, loss = 1.07751510\n",
      "Iteration 506, loss = 1.06779703\n",
      "Iteration 507, loss = 1.05719459\n",
      "Iteration 508, loss = 1.04726146\n",
      "Iteration 509, loss = 1.03827545\n",
      "Iteration 510, loss = 1.02812113\n",
      "Iteration 511, loss = 1.01832309\n",
      "Iteration 512, loss = 1.00915601\n",
      "Iteration 513, loss = 0.99939215\n",
      "Iteration 514, loss = 0.99096530\n",
      "Iteration 515, loss = 0.98209102\n",
      "Iteration 516, loss = 0.97292704\n",
      "Iteration 517, loss = 0.96331146\n",
      "Iteration 518, loss = 0.95497810\n",
      "Iteration 519, loss = 0.94653484\n",
      "Iteration 520, loss = 0.93934316\n",
      "Iteration 521, loss = 0.93087949\n",
      "Iteration 522, loss = 0.92137743\n",
      "Iteration 523, loss = 0.91322414\n",
      "Iteration 524, loss = 0.90589917\n",
      "Iteration 525, loss = 0.89739083\n",
      "Iteration 526, loss = 0.88928062\n",
      "Iteration 527, loss = 0.88215904\n",
      "Iteration 528, loss = 0.87489977\n",
      "Iteration 529, loss = 0.86713403\n",
      "Iteration 530, loss = 0.85977022\n",
      "Iteration 531, loss = 0.85268941\n",
      "Iteration 532, loss = 0.84510032\n",
      "Iteration 533, loss = 0.83803724\n",
      "Iteration 534, loss = 0.83114658\n",
      "Iteration 535, loss = 0.82399424\n",
      "Iteration 536, loss = 0.81809095\n",
      "Iteration 537, loss = 0.81182709\n",
      "Iteration 538, loss = 0.80544776\n",
      "Iteration 539, loss = 0.79741140\n",
      "Iteration 540, loss = 0.79138950\n",
      "Iteration 541, loss = 0.78481401\n",
      "Iteration 542, loss = 0.77964598\n",
      "Iteration 543, loss = 0.77344404\n",
      "Iteration 544, loss = 0.76983757\n",
      "Iteration 545, loss = 0.76147427\n",
      "Iteration 546, loss = 0.75442717\n",
      "Iteration 547, loss = 0.74885712\n",
      "Iteration 548, loss = 0.74290221\n",
      "Iteration 549, loss = 0.73723788\n",
      "Iteration 550, loss = 0.73208878\n",
      "Iteration 551, loss = 0.72749465\n",
      "Iteration 552, loss = 0.72072555\n",
      "Iteration 553, loss = 0.71557180\n",
      "Iteration 554, loss = 0.71124439\n",
      "Iteration 555, loss = 0.70601558\n",
      "Iteration 556, loss = 0.69929967\n",
      "Iteration 557, loss = 0.69453019\n",
      "Iteration 558, loss = 0.68984350\n",
      "Iteration 559, loss = 0.68526975\n",
      "Iteration 560, loss = 0.67996136\n",
      "Iteration 561, loss = 0.67448424\n",
      "Iteration 562, loss = 0.67022300\n",
      "Iteration 563, loss = 0.66540323\n",
      "Iteration 564, loss = 0.66084961\n",
      "Iteration 565, loss = 0.65611059\n",
      "Iteration 566, loss = 0.65188833\n",
      "Iteration 567, loss = 0.64545887\n",
      "Iteration 568, loss = 0.64362826\n",
      "Iteration 569, loss = 0.63760514\n",
      "Iteration 570, loss = 0.63449122\n",
      "Iteration 571, loss = 0.63182555\n",
      "Iteration 572, loss = 0.62362006\n",
      "Iteration 573, loss = 0.62076772\n",
      "Iteration 574, loss = 0.61709654\n",
      "Iteration 575, loss = 0.61297830\n",
      "Iteration 576, loss = 0.60865477\n",
      "Iteration 577, loss = 0.60444916\n",
      "Iteration 578, loss = 0.60133295\n",
      "Iteration 579, loss = 0.59729442\n",
      "Iteration 580, loss = 0.59364785\n",
      "Iteration 581, loss = 0.58965316\n",
      "Iteration 582, loss = 0.58630702\n",
      "Iteration 583, loss = 0.58229613\n",
      "Iteration 584, loss = 0.58107079\n",
      "Iteration 585, loss = 0.57668476\n",
      "Iteration 586, loss = 0.57267272\n",
      "Iteration 587, loss = 0.56927973\n",
      "Iteration 588, loss = 0.56731444\n",
      "Iteration 589, loss = 0.56353535\n",
      "Iteration 590, loss = 0.55903491\n",
      "Iteration 591, loss = 0.55609456\n",
      "Iteration 592, loss = 0.55349520\n",
      "Iteration 593, loss = 0.54942346\n",
      "Iteration 594, loss = 0.54898439\n",
      "Iteration 595, loss = 0.54587305\n",
      "Iteration 596, loss = 0.54207254\n",
      "Iteration 597, loss = 0.53936431\n",
      "Iteration 598, loss = 0.53655230\n",
      "Iteration 599, loss = 0.53240078\n",
      "Iteration 600, loss = 0.53014492\n",
      "Iteration 601, loss = 0.52840598\n",
      "Iteration 602, loss = 0.52479080\n",
      "Iteration 603, loss = 0.52223038\n",
      "Iteration 604, loss = 0.51947973\n",
      "Iteration 605, loss = 0.51704687\n",
      "Iteration 606, loss = 0.51584589\n",
      "Iteration 607, loss = 0.51352555\n",
      "Iteration 608, loss = 0.50992783\n",
      "Iteration 609, loss = 0.50872545\n",
      "Iteration 610, loss = 0.50532128\n",
      "Iteration 611, loss = 0.50254287\n",
      "Iteration 612, loss = 0.50093594\n",
      "Iteration 613, loss = 0.49898694\n",
      "Iteration 614, loss = 0.49603961\n",
      "Iteration 615, loss = 0.49345336\n",
      "Iteration 616, loss = 0.49175851\n",
      "Iteration 617, loss = 0.49001137\n",
      "Iteration 618, loss = 0.48761390\n",
      "Iteration 619, loss = 0.48745216\n",
      "Iteration 620, loss = 0.48256226\n",
      "Iteration 621, loss = 0.48098593\n",
      "Iteration 622, loss = 0.47843413\n",
      "Iteration 623, loss = 0.47646613\n",
      "Iteration 624, loss = 0.47557888\n",
      "Iteration 625, loss = 0.47298905\n",
      "Iteration 626, loss = 0.47144563\n",
      "Iteration 627, loss = 0.46902670\n",
      "Iteration 628, loss = 0.46925261\n",
      "Iteration 629, loss = 0.46727418\n",
      "Iteration 630, loss = 0.46340363\n",
      "Iteration 631, loss = 0.46143025\n",
      "Iteration 632, loss = 0.46052402\n",
      "Iteration 633, loss = 0.45776380\n",
      "Iteration 634, loss = 0.45555002\n",
      "Iteration 635, loss = 0.45365572\n",
      "Iteration 636, loss = 0.45251812\n",
      "Iteration 637, loss = 0.45052089\n",
      "Iteration 638, loss = 0.44962417\n",
      "Iteration 639, loss = 0.44655971\n",
      "Iteration 640, loss = 0.44508525\n",
      "Iteration 641, loss = 0.44403614\n",
      "Iteration 642, loss = 0.44159170\n",
      "Iteration 643, loss = 0.43916075\n",
      "Iteration 644, loss = 0.43901553\n",
      "Iteration 645, loss = 0.43744715\n",
      "Iteration 646, loss = 0.43384365\n",
      "Iteration 647, loss = 0.43328387\n",
      "Iteration 648, loss = 0.43179586\n",
      "Iteration 649, loss = 0.42931471\n",
      "Iteration 650, loss = 0.42733960\n",
      "Iteration 651, loss = 0.42501786\n",
      "Iteration 652, loss = 0.42369825\n",
      "Iteration 653, loss = 0.42394146\n",
      "Iteration 654, loss = 0.41992943\n",
      "Iteration 655, loss = 0.41955868\n",
      "Iteration 656, loss = 0.41717261\n",
      "Iteration 657, loss = 0.41457654\n",
      "Iteration 658, loss = 0.41478730\n",
      "Iteration 659, loss = 0.41327340\n",
      "Iteration 660, loss = 0.41268777\n",
      "Iteration 661, loss = 0.40927119\n",
      "Iteration 662, loss = 0.40797488\n",
      "Iteration 663, loss = 0.40627574\n",
      "Iteration 664, loss = 0.40442717\n",
      "Iteration 665, loss = 0.40431716\n",
      "Iteration 666, loss = 0.40505243\n",
      "Iteration 667, loss = 0.39995117\n",
      "Iteration 668, loss = 0.39916754\n",
      "Iteration 669, loss = 0.39764743\n",
      "Iteration 670, loss = 0.39584310\n",
      "Iteration 671, loss = 0.39390705\n",
      "Iteration 672, loss = 0.39345596\n",
      "Iteration 673, loss = 0.39086621\n",
      "Iteration 674, loss = 0.39019202\n",
      "Iteration 675, loss = 0.38781214\n",
      "Iteration 676, loss = 0.38604176\n",
      "Iteration 677, loss = 0.38471019\n",
      "Iteration 678, loss = 0.38429013\n",
      "Iteration 679, loss = 0.38292967\n",
      "Iteration 680, loss = 0.38142536\n",
      "Iteration 681, loss = 0.37986875\n",
      "Iteration 682, loss = 0.37983812\n",
      "Iteration 683, loss = 0.37756965\n",
      "Iteration 684, loss = 0.37501347\n",
      "Iteration 685, loss = 0.37660596\n",
      "Iteration 686, loss = 0.37243921\n",
      "Iteration 687, loss = 0.37189409\n",
      "Iteration 688, loss = 0.37029445\n",
      "Iteration 689, loss = 0.36886943\n",
      "Iteration 690, loss = 0.36732914\n",
      "Iteration 691, loss = 0.36605989\n",
      "Iteration 692, loss = 0.36441270\n",
      "Iteration 693, loss = 0.36311647\n",
      "Iteration 694, loss = 0.36235167\n",
      "Iteration 695, loss = 0.35950531\n",
      "Iteration 696, loss = 0.35986342\n",
      "Iteration 697, loss = 0.35854205\n",
      "Iteration 698, loss = 0.35735439\n",
      "Iteration 699, loss = 0.35537415\n",
      "Iteration 700, loss = 0.35384705\n",
      "Iteration 701, loss = 0.35260197\n",
      "Iteration 702, loss = 0.35119535\n",
      "Iteration 703, loss = 0.35089743\n",
      "Iteration 704, loss = 0.34925059\n",
      "Iteration 705, loss = 0.34765754\n",
      "Iteration 706, loss = 0.34693719\n",
      "Iteration 707, loss = 0.34594771\n",
      "Iteration 708, loss = 0.34359956\n",
      "Iteration 709, loss = 0.34347415\n",
      "Iteration 710, loss = 0.34107090\n",
      "Iteration 711, loss = 0.34057808\n",
      "Iteration 712, loss = 0.33969130\n",
      "Iteration 713, loss = 0.33874129\n",
      "Iteration 714, loss = 0.33620309\n",
      "Iteration 715, loss = 0.33512836\n",
      "Iteration 716, loss = 0.33368159\n",
      "Iteration 717, loss = 0.33305487\n",
      "Iteration 718, loss = 0.33283821\n",
      "Iteration 719, loss = 0.33043291\n",
      "Iteration 720, loss = 0.32942701\n",
      "Iteration 721, loss = 0.32793910\n",
      "Iteration 722, loss = 0.32678548\n",
      "Iteration 723, loss = 0.32621048\n",
      "Iteration 724, loss = 0.32483524\n",
      "Iteration 725, loss = 0.32357865\n",
      "Iteration 726, loss = 0.32263778\n",
      "Iteration 727, loss = 0.32228683\n",
      "Iteration 728, loss = 0.31966110\n",
      "Iteration 729, loss = 0.31837445\n",
      "Iteration 730, loss = 0.31748683\n",
      "Iteration 731, loss = 0.31644849\n",
      "Iteration 732, loss = 0.31549781\n",
      "Iteration 733, loss = 0.31563105\n",
      "Iteration 734, loss = 0.31358542\n",
      "Iteration 735, loss = 0.31318503\n",
      "Iteration 736, loss = 0.31079677\n",
      "Iteration 737, loss = 0.31037154\n",
      "Iteration 738, loss = 0.30991431\n",
      "Iteration 739, loss = 0.30823628\n",
      "Iteration 740, loss = 0.30776269\n",
      "Iteration 741, loss = 0.30677582\n",
      "Iteration 742, loss = 0.30443954\n",
      "Iteration 743, loss = 0.30351361\n",
      "Iteration 744, loss = 0.30289976\n",
      "Iteration 745, loss = 0.30105938\n",
      "Iteration 746, loss = 0.30165507\n",
      "Iteration 747, loss = 0.29945113\n",
      "Iteration 748, loss = 0.29849330\n",
      "Iteration 749, loss = 0.29771080\n",
      "Iteration 750, loss = 0.29620834\n",
      "Iteration 751, loss = 0.29562788\n",
      "Iteration 752, loss = 0.29502865\n",
      "Iteration 753, loss = 0.29359782\n",
      "Iteration 754, loss = 0.29283899\n",
      "Iteration 755, loss = 0.29196096\n",
      "Iteration 756, loss = 0.29143203\n",
      "Iteration 757, loss = 0.28945424\n",
      "Iteration 758, loss = 0.28953897\n",
      "Iteration 759, loss = 0.28802690\n",
      "Iteration 760, loss = 0.28721910\n",
      "Iteration 761, loss = 0.28652857\n",
      "Iteration 762, loss = 0.28550735\n",
      "Iteration 763, loss = 0.28402901\n",
      "Iteration 764, loss = 0.28438100\n",
      "Iteration 765, loss = 0.28219822\n",
      "Iteration 766, loss = 0.28148553\n",
      "Iteration 767, loss = 0.28064386\n",
      "Iteration 768, loss = 0.27936129\n",
      "Iteration 769, loss = 0.27868774\n",
      "Iteration 770, loss = 0.27787971\n",
      "Iteration 771, loss = 0.27826902\n",
      "Iteration 772, loss = 0.27727671\n",
      "Iteration 773, loss = 0.27685343\n",
      "Iteration 774, loss = 0.27438384\n",
      "Iteration 775, loss = 0.27397009\n",
      "Iteration 776, loss = 0.27336615\n",
      "Iteration 777, loss = 0.27119834\n",
      "Iteration 778, loss = 0.27161869\n",
      "Iteration 779, loss = 0.27073915\n",
      "Iteration 780, loss = 0.26917771\n",
      "Iteration 781, loss = 0.26884258\n",
      "Iteration 782, loss = 0.26743206\n",
      "Iteration 783, loss = 0.26650270\n",
      "Iteration 784, loss = 0.26576499\n",
      "Iteration 785, loss = 0.26848466\n",
      "Iteration 786, loss = 0.26445113\n",
      "Iteration 787, loss = 0.26346822\n",
      "Iteration 788, loss = 0.26238758\n",
      "Iteration 789, loss = 0.26153574\n",
      "Iteration 790, loss = 0.26189226\n",
      "Iteration 791, loss = 0.26034427\n",
      "Iteration 792, loss = 0.25905727\n",
      "Iteration 793, loss = 0.25853990\n",
      "Iteration 794, loss = 0.25763869\n",
      "Iteration 795, loss = 0.25720937\n",
      "Iteration 796, loss = 0.25683774\n",
      "Iteration 797, loss = 0.25492585\n",
      "Iteration 798, loss = 0.25484676\n",
      "Iteration 799, loss = 0.25455830\n",
      "Iteration 800, loss = 0.25383732\n",
      "Iteration 801, loss = 0.25278544\n",
      "Iteration 802, loss = 0.25147659\n",
      "Iteration 803, loss = 0.25043365\n",
      "Iteration 804, loss = 0.24940587\n",
      "Iteration 805, loss = 0.24856870\n",
      "Iteration 806, loss = 0.24775356\n",
      "Iteration 807, loss = 0.24789115\n",
      "Iteration 808, loss = 0.24680926\n",
      "Iteration 809, loss = 0.24495253\n",
      "Iteration 810, loss = 0.24468535\n",
      "Iteration 811, loss = 0.24407806\n",
      "Iteration 812, loss = 0.24427410\n",
      "Iteration 813, loss = 0.24288446\n",
      "Iteration 814, loss = 0.24368591\n",
      "Iteration 815, loss = 0.24178925\n",
      "Iteration 816, loss = 0.24081989\n",
      "Iteration 817, loss = 0.23944022\n",
      "Iteration 818, loss = 0.23955067\n",
      "Iteration 819, loss = 0.23790538\n",
      "Iteration 820, loss = 0.23747603\n",
      "Iteration 821, loss = 0.23750448\n",
      "Iteration 822, loss = 0.23563272\n",
      "Iteration 823, loss = 0.23515332\n",
      "Iteration 824, loss = 0.23457031\n",
      "Iteration 825, loss = 0.23369655\n",
      "Iteration 826, loss = 0.23292036\n",
      "Iteration 827, loss = 0.23166894\n",
      "Iteration 828, loss = 0.23144948\n",
      "Iteration 829, loss = 0.23131232\n",
      "Iteration 830, loss = 0.23005940\n",
      "Iteration 831, loss = 0.22991123\n",
      "Iteration 832, loss = 0.22815403\n",
      "Iteration 833, loss = 0.22758644\n",
      "Iteration 834, loss = 0.22692603\n",
      "Iteration 835, loss = 0.22634036\n",
      "Iteration 836, loss = 0.22672396\n",
      "Iteration 837, loss = 0.22458672\n",
      "Iteration 838, loss = 0.22438102\n",
      "Iteration 839, loss = 0.22367779\n",
      "Iteration 840, loss = 0.22271898\n",
      "Iteration 841, loss = 0.22153663\n",
      "Iteration 842, loss = 0.22112128\n",
      "Iteration 843, loss = 0.22062815\n",
      "Iteration 844, loss = 0.21944388\n",
      "Iteration 845, loss = 0.21926434\n",
      "Iteration 846, loss = 0.21855261\n",
      "Iteration 847, loss = 0.21883789\n",
      "Iteration 848, loss = 0.21688679\n",
      "Iteration 849, loss = 0.21606381\n",
      "Iteration 850, loss = 0.21493885\n",
      "Iteration 851, loss = 0.21489748\n",
      "Iteration 852, loss = 0.21400204\n",
      "Iteration 853, loss = 0.21291066\n",
      "Iteration 854, loss = 0.21327315\n",
      "Iteration 855, loss = 0.21256017\n",
      "Iteration 856, loss = 0.21201433\n",
      "Iteration 857, loss = 0.21068971\n",
      "Iteration 858, loss = 0.21045270\n",
      "Iteration 859, loss = 0.21004179\n",
      "Iteration 860, loss = 0.20897859\n",
      "Iteration 861, loss = 0.20798199\n",
      "Iteration 862, loss = 0.20730406\n",
      "Iteration 863, loss = 0.20739945\n",
      "Iteration 864, loss = 0.20612838\n",
      "Iteration 865, loss = 0.20490949\n",
      "Iteration 866, loss = 0.20562783\n",
      "Iteration 867, loss = 0.20445128\n",
      "Iteration 868, loss = 0.20427529\n",
      "Iteration 869, loss = 0.20327629\n",
      "Iteration 870, loss = 0.20197821\n",
      "Iteration 871, loss = 0.20182086\n",
      "Iteration 872, loss = 0.20105244\n",
      "Iteration 873, loss = 0.19994992\n",
      "Iteration 874, loss = 0.19920773\n",
      "Iteration 875, loss = 0.20054224\n",
      "Iteration 876, loss = 0.19882498\n",
      "Iteration 877, loss = 0.19820365\n",
      "Iteration 878, loss = 0.19677468\n",
      "Iteration 879, loss = 0.19678688\n",
      "Iteration 880, loss = 0.19520129\n",
      "Iteration 881, loss = 0.19532062\n",
      "Iteration 882, loss = 0.19457315\n",
      "Iteration 883, loss = 0.19367798\n",
      "Iteration 884, loss = 0.19336157\n",
      "Iteration 885, loss = 0.19230033\n",
      "Iteration 886, loss = 0.19205500\n",
      "Iteration 887, loss = 0.19121083\n",
      "Iteration 888, loss = 0.19087202\n",
      "Iteration 889, loss = 0.19142754\n",
      "Iteration 890, loss = 0.19093355\n",
      "Iteration 891, loss = 0.18872663\n",
      "Iteration 892, loss = 0.18849740\n",
      "Iteration 893, loss = 0.18743762\n",
      "Iteration 894, loss = 0.18790993\n",
      "Iteration 895, loss = 0.18631017\n",
      "Iteration 896, loss = 0.18600654\n",
      "Iteration 897, loss = 0.18518527\n",
      "Iteration 898, loss = 0.18438227\n",
      "Iteration 899, loss = 0.18456580\n",
      "Iteration 900, loss = 0.18348243\n",
      "Iteration 901, loss = 0.18235944\n",
      "Iteration 902, loss = 0.18210090\n",
      "Iteration 903, loss = 0.18183582\n",
      "Iteration 904, loss = 0.18146037\n",
      "Iteration 905, loss = 0.18087045\n",
      "Iteration 906, loss = 0.18012570\n",
      "Iteration 907, loss = 0.17927972\n",
      "Iteration 908, loss = 0.17826654\n",
      "Iteration 909, loss = 0.17790765\n",
      "Iteration 910, loss = 0.17720385\n",
      "Iteration 911, loss = 0.17664934\n",
      "Iteration 912, loss = 0.17654535\n",
      "Iteration 913, loss = 0.17520422\n",
      "Iteration 914, loss = 0.17590040\n",
      "Iteration 915, loss = 0.17510578\n",
      "Iteration 916, loss = 0.17377487\n",
      "Iteration 917, loss = 0.17396775\n",
      "Iteration 918, loss = 0.17300469\n",
      "Iteration 919, loss = 0.17173123\n",
      "Iteration 920, loss = 0.17172214\n",
      "Iteration 921, loss = 0.17125777\n",
      "Iteration 922, loss = 0.17009510\n",
      "Iteration 923, loss = 0.17140291\n",
      "Iteration 924, loss = 0.16973961\n",
      "Iteration 925, loss = 0.16920119\n",
      "Iteration 926, loss = 0.16830564\n",
      "Iteration 927, loss = 0.16866373\n",
      "Iteration 928, loss = 0.16897598\n",
      "Iteration 929, loss = 0.16734822\n",
      "Iteration 930, loss = 0.16579073\n",
      "Iteration 931, loss = 0.16617576\n",
      "Iteration 932, loss = 0.16516549\n",
      "Iteration 933, loss = 0.16387695\n",
      "Iteration 934, loss = 0.16425069\n",
      "Iteration 935, loss = 0.16363849\n",
      "Iteration 936, loss = 0.16307957\n",
      "Iteration 937, loss = 0.16204662\n",
      "Iteration 938, loss = 0.16166004\n",
      "Iteration 939, loss = 0.16106065\n",
      "Iteration 940, loss = 0.16065258\n",
      "Iteration 941, loss = 0.16035599\n",
      "Iteration 942, loss = 0.15989876\n",
      "Iteration 943, loss = 0.16039630\n",
      "Iteration 944, loss = 0.15830374\n",
      "Iteration 945, loss = 0.15824377\n",
      "Iteration 946, loss = 0.15792354\n",
      "Iteration 947, loss = 0.15701235\n",
      "Iteration 948, loss = 0.15653354\n",
      "Iteration 949, loss = 0.15618197\n",
      "Iteration 950, loss = 0.15593817\n",
      "Iteration 951, loss = 0.15522738\n",
      "Iteration 952, loss = 0.15413093\n",
      "Iteration 953, loss = 0.15431728\n",
      "Iteration 954, loss = 0.15369771\n",
      "Iteration 955, loss = 0.15326334\n",
      "Iteration 956, loss = 0.15229588\n",
      "Iteration 957, loss = 0.15265383\n",
      "Iteration 958, loss = 0.15212715\n",
      "Iteration 959, loss = 0.15052979\n",
      "Iteration 960, loss = 0.15080464\n",
      "Iteration 961, loss = 0.14966402\n",
      "Iteration 962, loss = 0.14928550\n",
      "Iteration 963, loss = 0.15054380\n",
      "Iteration 964, loss = 0.14826348\n",
      "Iteration 965, loss = 0.14896049\n",
      "Iteration 966, loss = 0.14743762\n",
      "Iteration 967, loss = 0.14700447\n",
      "Iteration 968, loss = 0.14615333\n",
      "Iteration 969, loss = 0.14637874\n",
      "Iteration 970, loss = 0.14571113\n",
      "Iteration 971, loss = 0.14547527\n",
      "Iteration 972, loss = 0.14436514\n",
      "Iteration 973, loss = 0.14393162\n",
      "Iteration 974, loss = 0.14337080\n",
      "Iteration 975, loss = 0.14425399\n",
      "Iteration 976, loss = 0.14278318\n",
      "Iteration 977, loss = 0.14192353\n",
      "Iteration 978, loss = 0.14144740\n",
      "Iteration 979, loss = 0.14116828\n",
      "Iteration 980, loss = 0.14081344\n",
      "Iteration 981, loss = 0.14002783\n",
      "Iteration 982, loss = 0.13976557\n",
      "Iteration 983, loss = 0.13927407\n",
      "Iteration 984, loss = 0.13988543\n",
      "Iteration 985, loss = 0.13938771\n",
      "Iteration 986, loss = 0.13817410\n",
      "Iteration 987, loss = 0.13729958\n",
      "Iteration 988, loss = 0.13726621\n",
      "Iteration 989, loss = 0.13653365\n",
      "Iteration 990, loss = 0.13626488\n",
      "Iteration 991, loss = 0.13610788\n",
      "Iteration 992, loss = 0.13608312\n",
      "Iteration 993, loss = 0.13501517\n",
      "Iteration 994, loss = 0.13491420\n",
      "Iteration 995, loss = 0.13456390\n",
      "Iteration 996, loss = 0.13382536\n",
      "Iteration 997, loss = 0.13329873\n",
      "Iteration 998, loss = 0.13287644\n",
      "Iteration 999, loss = 0.13277978\n",
      "Iteration 1000, loss = 0.13144648\n",
      "Iteration 1001, loss = 0.13128643\n",
      "Iteration 1002, loss = 0.13124579\n",
      "Iteration 1003, loss = 0.13072846\n",
      "Iteration 1004, loss = 0.13027416\n",
      "Iteration 1005, loss = 0.12991422\n",
      "Iteration 1006, loss = 0.12934243\n",
      "Iteration 1007, loss = 0.12846322\n",
      "Iteration 1008, loss = 0.12883915\n",
      "Iteration 1009, loss = 0.12788063\n",
      "Iteration 1010, loss = 0.12836222\n",
      "Iteration 1011, loss = 0.12780134\n",
      "Iteration 1012, loss = 0.12669742\n",
      "Iteration 1013, loss = 0.12641686\n",
      "Iteration 1014, loss = 0.12610832\n",
      "Iteration 1015, loss = 0.12628848\n",
      "Iteration 1016, loss = 0.12540414\n",
      "Iteration 1017, loss = 0.12447015\n",
      "Iteration 1018, loss = 0.12420223\n",
      "Iteration 1019, loss = 0.12375627\n",
      "Iteration 1020, loss = 0.12359004\n",
      "Iteration 1021, loss = 0.12359328\n",
      "Iteration 1022, loss = 0.12238762\n",
      "Iteration 1023, loss = 0.12205766\n",
      "Iteration 1024, loss = 0.12200874\n",
      "Iteration 1025, loss = 0.12150557\n",
      "Iteration 1026, loss = 0.12122385\n",
      "Iteration 1027, loss = 0.12081771\n",
      "Iteration 1028, loss = 0.12072092\n",
      "Iteration 1029, loss = 0.11973229\n",
      "Iteration 1030, loss = 0.12001849\n",
      "Iteration 1031, loss = 0.11898080\n",
      "Iteration 1032, loss = 0.11870018\n",
      "Iteration 1033, loss = 0.11830410\n",
      "Iteration 1034, loss = 0.11780571\n",
      "Iteration 1035, loss = 0.11878099\n",
      "Iteration 1036, loss = 0.11742740\n",
      "Iteration 1037, loss = 0.11680535\n",
      "Iteration 1038, loss = 0.11641104\n",
      "Iteration 1039, loss = 0.11600060\n",
      "Iteration 1040, loss = 0.11540406\n",
      "Iteration 1041, loss = 0.11550707\n",
      "Iteration 1042, loss = 0.11464322\n",
      "Iteration 1043, loss = 0.11461587\n",
      "Iteration 1044, loss = 0.11439282\n",
      "Iteration 1045, loss = 0.11369446\n",
      "Iteration 1046, loss = 0.11319312\n",
      "Iteration 1047, loss = 0.11301775\n",
      "Iteration 1048, loss = 0.11304139\n",
      "Iteration 1049, loss = 0.11204099\n",
      "Iteration 1050, loss = 0.11186605\n",
      "Iteration 1051, loss = 0.11198244\n",
      "Iteration 1052, loss = 0.11140765\n",
      "Iteration 1053, loss = 0.11144426\n",
      "Iteration 1054, loss = 0.11012351\n",
      "Iteration 1055, loss = 0.11007042\n",
      "Iteration 1056, loss = 0.10966415\n",
      "Iteration 1057, loss = 0.10957379\n",
      "Iteration 1058, loss = 0.10872393\n",
      "Iteration 1059, loss = 0.10885476\n",
      "Iteration 1060, loss = 0.10822605\n",
      "Iteration 1061, loss = 0.10797056\n",
      "Iteration 1062, loss = 0.10755914\n",
      "Iteration 1063, loss = 0.10710046\n",
      "Iteration 1064, loss = 0.10676653\n",
      "Iteration 1065, loss = 0.10625231\n",
      "Iteration 1066, loss = 0.10633546\n",
      "Iteration 1067, loss = 0.10583399\n",
      "Iteration 1068, loss = 0.10525810\n",
      "Iteration 1069, loss = 0.10527935\n",
      "Iteration 1070, loss = 0.10486936\n",
      "Iteration 1071, loss = 0.10418666\n",
      "Iteration 1072, loss = 0.10433871\n",
      "Iteration 1073, loss = 0.10437466\n",
      "Iteration 1074, loss = 0.10312171\n",
      "Iteration 1075, loss = 0.10380732\n",
      "Iteration 1076, loss = 0.10354535\n",
      "Iteration 1077, loss = 0.10242134\n",
      "Iteration 1078, loss = 0.10177027\n",
      "Iteration 1079, loss = 0.10237550\n",
      "Iteration 1080, loss = 0.10169507\n",
      "Iteration 1081, loss = 0.10099297\n",
      "Iteration 1082, loss = 0.10133861\n",
      "Iteration 1083, loss = 0.10058921\n",
      "Iteration 1084, loss = 0.09998471\n",
      "Iteration 1085, loss = 0.09960578\n",
      "Iteration 1086, loss = 0.09993595\n",
      "Iteration 1087, loss = 0.09926189\n",
      "Iteration 1088, loss = 0.09859947\n",
      "Iteration 1089, loss = 0.09853801\n",
      "Iteration 1090, loss = 0.09790318\n",
      "Iteration 1091, loss = 0.09769578\n",
      "Iteration 1092, loss = 0.09729513\n",
      "Iteration 1093, loss = 0.09762061\n",
      "Iteration 1094, loss = 0.09689736\n",
      "Iteration 1095, loss = 0.09689408\n",
      "Iteration 1096, loss = 0.09649844\n",
      "Iteration 1097, loss = 0.09627897\n",
      "Iteration 1098, loss = 0.09550086\n",
      "Iteration 1099, loss = 0.09589536\n",
      "Iteration 1100, loss = 0.09496917\n",
      "Iteration 1101, loss = 0.09454239\n",
      "Iteration 1102, loss = 0.09420641\n",
      "Iteration 1103, loss = 0.09381878\n",
      "Iteration 1104, loss = 0.09363206\n",
      "Iteration 1105, loss = 0.09367433\n",
      "Iteration 1106, loss = 0.09325314\n",
      "Iteration 1107, loss = 0.09281860\n",
      "Iteration 1108, loss = 0.09246087\n",
      "Iteration 1109, loss = 0.09287050\n",
      "Iteration 1110, loss = 0.09198983\n",
      "Iteration 1111, loss = 0.09160484\n",
      "Iteration 1112, loss = 0.09211046\n",
      "Iteration 1113, loss = 0.09073946\n",
      "Iteration 1114, loss = 0.09097048\n",
      "Iteration 1115, loss = 0.09113600\n",
      "Iteration 1116, loss = 0.09046416\n",
      "Iteration 1117, loss = 0.09005032\n",
      "Iteration 1118, loss = 0.08949090\n",
      "Iteration 1119, loss = 0.08989591\n",
      "Iteration 1120, loss = 0.08904428\n",
      "Iteration 1121, loss = 0.08864204\n",
      "Iteration 1122, loss = 0.08916851\n",
      "Iteration 1123, loss = 0.08841398\n",
      "Iteration 1124, loss = 0.08781419\n",
      "Iteration 1125, loss = 0.08793052\n",
      "Iteration 1126, loss = 0.08749935\n",
      "Iteration 1127, loss = 0.08736497\n",
      "Iteration 1128, loss = 0.08719464\n",
      "Iteration 1129, loss = 0.08688879\n",
      "Iteration 1130, loss = 0.08643467\n",
      "Iteration 1131, loss = 0.08600064\n",
      "Iteration 1132, loss = 0.08627962\n",
      "Iteration 1133, loss = 0.08567917\n",
      "Iteration 1134, loss = 0.08532038\n",
      "Iteration 1135, loss = 0.08511558\n",
      "Iteration 1136, loss = 0.08453687\n",
      "Iteration 1137, loss = 0.08419746\n",
      "Iteration 1138, loss = 0.08437186\n",
      "Iteration 1139, loss = 0.08372686\n",
      "Iteration 1140, loss = 0.08353648\n",
      "Iteration 1141, loss = 0.08331464\n",
      "Iteration 1142, loss = 0.08316051\n",
      "Iteration 1143, loss = 0.08239906\n",
      "Iteration 1144, loss = 0.08256674\n",
      "Iteration 1145, loss = 0.08260176\n",
      "Iteration 1146, loss = 0.08193230\n",
      "Iteration 1147, loss = 0.08174402\n",
      "Iteration 1148, loss = 0.08156010\n",
      "Iteration 1149, loss = 0.08165421\n",
      "Iteration 1150, loss = 0.08139232\n",
      "Iteration 1151, loss = 0.08061246\n",
      "Iteration 1152, loss = 0.08023947\n",
      "Iteration 1153, loss = 0.08021856\n",
      "Iteration 1154, loss = 0.07971934\n",
      "Iteration 1155, loss = 0.07965746\n",
      "Iteration 1156, loss = 0.07983409\n",
      "Iteration 1157, loss = 0.07951675\n",
      "Iteration 1158, loss = 0.07908387\n",
      "Iteration 1159, loss = 0.07915875\n",
      "Iteration 1160, loss = 0.07843144\n",
      "Iteration 1161, loss = 0.07795844\n",
      "Iteration 1162, loss = 0.07903595\n",
      "Iteration 1163, loss = 0.07793865\n",
      "Iteration 1164, loss = 0.07748569\n",
      "Iteration 1165, loss = 0.07745646\n",
      "Iteration 1166, loss = 0.07706475\n",
      "Iteration 1167, loss = 0.07694134\n",
      "Iteration 1168, loss = 0.07661750\n",
      "Iteration 1169, loss = 0.07625109\n",
      "Iteration 1170, loss = 0.07637234\n",
      "Iteration 1171, loss = 0.07572391\n",
      "Iteration 1172, loss = 0.07574029\n",
      "Iteration 1173, loss = 0.07570935\n",
      "Iteration 1174, loss = 0.07511705\n",
      "Iteration 1175, loss = 0.07472332\n",
      "Iteration 1176, loss = 0.07517685\n",
      "Iteration 1177, loss = 0.07439917\n",
      "Iteration 1178, loss = 0.07468820\n",
      "Iteration 1179, loss = 0.07409262\n",
      "Iteration 1180, loss = 0.07392122\n",
      "Iteration 1181, loss = 0.07376145\n",
      "Iteration 1182, loss = 0.07324724\n",
      "Iteration 1183, loss = 0.07306724\n",
      "Iteration 1184, loss = 0.07338471\n",
      "Iteration 1185, loss = 0.07264292\n",
      "Iteration 1186, loss = 0.07239980\n",
      "Iteration 1187, loss = 0.07262711\n",
      "Iteration 1188, loss = 0.07227999\n",
      "Iteration 1189, loss = 0.07195890\n",
      "Iteration 1190, loss = 0.07186218\n",
      "Iteration 1191, loss = 0.07130750\n",
      "Iteration 1192, loss = 0.07116935\n",
      "Iteration 1193, loss = 0.07110278\n",
      "Iteration 1194, loss = 0.07095020\n",
      "Iteration 1195, loss = 0.07144398\n",
      "Iteration 1196, loss = 0.07061396\n",
      "Iteration 1197, loss = 0.07006024\n",
      "Iteration 1198, loss = 0.06978232\n",
      "Iteration 1199, loss = 0.06976539\n",
      "Iteration 1200, loss = 0.06948108\n",
      "Iteration 1201, loss = 0.06945856\n",
      "Iteration 1202, loss = 0.06924051\n",
      "Iteration 1203, loss = 0.06903534\n",
      "Iteration 1204, loss = 0.06877194\n",
      "Iteration 1205, loss = 0.06890408\n",
      "Iteration 1206, loss = 0.06890620\n",
      "Iteration 1207, loss = 0.06826684\n",
      "Iteration 1208, loss = 0.06838759\n",
      "Iteration 1209, loss = 0.06845982\n",
      "Iteration 1210, loss = 0.06773827\n",
      "Iteration 1211, loss = 0.06802511\n",
      "Iteration 1212, loss = 0.06744299\n",
      "Iteration 1213, loss = 0.06683719\n",
      "Iteration 1214, loss = 0.06696825\n",
      "Iteration 1215, loss = 0.06661465\n",
      "Iteration 1216, loss = 0.06661729\n",
      "Iteration 1217, loss = 0.06622114\n",
      "Iteration 1218, loss = 0.06614646\n",
      "Iteration 1219, loss = 0.06608841\n",
      "Iteration 1220, loss = 0.06591750\n",
      "Iteration 1221, loss = 0.06544670\n",
      "Iteration 1222, loss = 0.06608679\n",
      "Iteration 1223, loss = 0.06560135\n",
      "Iteration 1224, loss = 0.06493289\n",
      "Iteration 1225, loss = 0.06469107\n",
      "Iteration 1226, loss = 0.06460031\n",
      "Iteration 1227, loss = 0.06477542\n",
      "Iteration 1228, loss = 0.06537947\n",
      "Iteration 1229, loss = 0.06401514\n",
      "Iteration 1230, loss = 0.06381424\n",
      "Iteration 1231, loss = 0.06410083\n",
      "Iteration 1232, loss = 0.06475814\n",
      "Iteration 1233, loss = 0.06363729\n",
      "Iteration 1234, loss = 0.06329448\n",
      "Iteration 1235, loss = 0.06311814\n",
      "Iteration 1236, loss = 0.06348218\n",
      "Iteration 1237, loss = 0.06277719\n",
      "Iteration 1238, loss = 0.06230649\n",
      "Iteration 1239, loss = 0.06230936\n",
      "Iteration 1240, loss = 0.06224338\n",
      "Iteration 1241, loss = 0.06192917\n",
      "Iteration 1242, loss = 0.06210258\n",
      "Iteration 1243, loss = 0.06184732\n",
      "Iteration 1244, loss = 0.06185782\n",
      "Iteration 1245, loss = 0.06153977\n",
      "Iteration 1246, loss = 0.06127919\n",
      "Iteration 1247, loss = 0.06099575\n",
      "Iteration 1248, loss = 0.06150668\n",
      "Iteration 1249, loss = 0.06111929\n",
      "Iteration 1250, loss = 0.06041697\n",
      "Iteration 1251, loss = 0.06051333\n",
      "Iteration 1252, loss = 0.06008755\n",
      "Iteration 1253, loss = 0.06003837\n",
      "Iteration 1254, loss = 0.06020046\n",
      "Iteration 1255, loss = 0.05979432\n",
      "Iteration 1256, loss = 0.05934426\n",
      "Iteration 1257, loss = 0.05938146\n",
      "Iteration 1258, loss = 0.05937631\n",
      "Iteration 1259, loss = 0.05958087\n",
      "Iteration 1260, loss = 0.06020266\n",
      "Iteration 1261, loss = 0.05886067\n",
      "Iteration 1262, loss = 0.05859542\n",
      "Iteration 1263, loss = 0.05868538\n",
      "Iteration 1264, loss = 0.05831907\n",
      "Iteration 1265, loss = 0.05817627\n",
      "Iteration 1266, loss = 0.05822083\n",
      "Iteration 1267, loss = 0.05803583\n",
      "Iteration 1268, loss = 0.05789881\n",
      "Iteration 1269, loss = 0.05779641\n",
      "Iteration 1270, loss = 0.05735089\n",
      "Iteration 1271, loss = 0.05721145\n",
      "Iteration 1272, loss = 0.05721356\n",
      "Iteration 1273, loss = 0.05687089\n",
      "Iteration 1274, loss = 0.05714978\n",
      "Iteration 1275, loss = 0.05667410\n",
      "Iteration 1276, loss = 0.05647890\n",
      "Iteration 1277, loss = 0.05674090\n",
      "Iteration 1278, loss = 0.05631076\n",
      "Iteration 1279, loss = 0.05627595\n",
      "Iteration 1280, loss = 0.05602470\n",
      "Iteration 1281, loss = 0.05572775\n",
      "Iteration 1282, loss = 0.05554383\n",
      "Iteration 1283, loss = 0.05557457\n",
      "Iteration 1284, loss = 0.05568985\n",
      "Iteration 1285, loss = 0.05544244\n",
      "Iteration 1286, loss = 0.05499058\n",
      "Iteration 1287, loss = 0.05514399\n",
      "Iteration 1288, loss = 0.05534798\n",
      "Iteration 1289, loss = 0.05478778\n",
      "Iteration 1290, loss = 0.05458200\n",
      "Iteration 1291, loss = 0.05439170\n",
      "Iteration 1292, loss = 0.05447236\n",
      "Iteration 1293, loss = 0.05443723\n",
      "Iteration 1294, loss = 0.05420800\n",
      "Iteration 1295, loss = 0.05428250\n",
      "Iteration 1296, loss = 0.05377974\n",
      "Iteration 1297, loss = 0.05375475\n",
      "Iteration 1298, loss = 0.05356791\n",
      "Iteration 1299, loss = 0.05336460\n",
      "Iteration 1300, loss = 0.05331442\n",
      "Iteration 1301, loss = 0.05295760\n",
      "Iteration 1302, loss = 0.05321968\n",
      "Iteration 1303, loss = 0.05287467\n",
      "Iteration 1304, loss = 0.05293465\n",
      "Iteration 1305, loss = 0.05254201\n",
      "Iteration 1306, loss = 0.05275867\n",
      "Iteration 1307, loss = 0.05234196\n",
      "Iteration 1308, loss = 0.05231510\n",
      "Iteration 1309, loss = 0.05210859\n",
      "Iteration 1310, loss = 0.05201019\n",
      "Iteration 1311, loss = 0.05205182\n",
      "Iteration 1312, loss = 0.05178474\n",
      "Iteration 1313, loss = 0.05156972\n",
      "Iteration 1314, loss = 0.05177969\n",
      "Iteration 1315, loss = 0.05215297\n",
      "Iteration 1316, loss = 0.05165020\n",
      "Iteration 1317, loss = 0.05117830\n",
      "Iteration 1318, loss = 0.05147581\n",
      "Iteration 1319, loss = 0.05091442\n",
      "Iteration 1320, loss = 0.05073542\n",
      "Iteration 1321, loss = 0.05124796\n",
      "Iteration 1322, loss = 0.05089475\n",
      "Iteration 1323, loss = 0.05041588\n",
      "Iteration 1324, loss = 0.05055458\n",
      "Iteration 1325, loss = 0.05011142\n",
      "Iteration 1326, loss = 0.04998121\n",
      "Iteration 1327, loss = 0.05030470\n",
      "Iteration 1328, loss = 0.04978046\n",
      "Iteration 1329, loss = 0.04987350\n",
      "Iteration 1330, loss = 0.05005065\n",
      "Iteration 1331, loss = 0.04963016\n",
      "Iteration 1332, loss = 0.04936480\n",
      "Iteration 1333, loss = 0.04960114\n",
      "Iteration 1334, loss = 0.04937859\n",
      "Iteration 1335, loss = 0.04923317\n",
      "Iteration 1336, loss = 0.04959901\n",
      "Iteration 1337, loss = 0.04914156\n",
      "Iteration 1338, loss = 0.04878989\n",
      "Iteration 1339, loss = 0.04935710\n",
      "Iteration 1340, loss = 0.04871712\n",
      "Iteration 1341, loss = 0.04847126\n",
      "Iteration 1342, loss = 0.04878130\n",
      "Iteration 1343, loss = 0.04815967\n",
      "Iteration 1344, loss = 0.04817690\n",
      "Iteration 1345, loss = 0.04848403\n",
      "Iteration 1346, loss = 0.04813420\n",
      "Iteration 1347, loss = 0.04782423\n",
      "Iteration 1348, loss = 0.04792257\n",
      "Iteration 1349, loss = 0.04790271\n",
      "Iteration 1350, loss = 0.04796370\n",
      "Iteration 1351, loss = 0.04748228\n",
      "Iteration 1352, loss = 0.04729157\n",
      "Iteration 1353, loss = 0.04728869\n",
      "Iteration 1354, loss = 0.04706661\n",
      "Iteration 1355, loss = 0.04711626\n",
      "Iteration 1356, loss = 0.04685805\n",
      "Iteration 1357, loss = 0.04697999\n",
      "Iteration 1358, loss = 0.04686141\n",
      "Iteration 1359, loss = 0.04660066\n",
      "Iteration 1360, loss = 0.04647529\n",
      "Iteration 1361, loss = 0.04637553\n",
      "Iteration 1362, loss = 0.04647397\n",
      "Iteration 1363, loss = 0.04634403\n",
      "Iteration 1364, loss = 0.04610079\n",
      "Iteration 1365, loss = 0.04624824\n",
      "Iteration 1366, loss = 0.04617321\n",
      "Iteration 1367, loss = 0.04594450\n",
      "Iteration 1368, loss = 0.04565474\n",
      "Iteration 1369, loss = 0.04581038\n",
      "Iteration 1370, loss = 0.04557300\n",
      "Iteration 1371, loss = 0.04556357\n",
      "Iteration 1372, loss = 0.04539631\n",
      "Iteration 1373, loss = 0.04544222\n",
      "Iteration 1374, loss = 0.04508232\n",
      "Iteration 1375, loss = 0.04522149\n",
      "Iteration 1376, loss = 0.04512779\n",
      "Iteration 1377, loss = 0.04507342\n",
      "Iteration 1378, loss = 0.04498672\n",
      "Iteration 1379, loss = 0.04495345\n",
      "Iteration 1380, loss = 0.04465946\n",
      "Iteration 1381, loss = 0.04470621\n",
      "Iteration 1382, loss = 0.04443775\n",
      "Iteration 1383, loss = 0.04469828\n",
      "Iteration 1384, loss = 0.04468788\n",
      "Iteration 1385, loss = 0.04445852\n",
      "Iteration 1386, loss = 0.04421529\n",
      "Iteration 1387, loss = 0.04402456\n",
      "Iteration 1388, loss = 0.04427660\n",
      "Iteration 1389, loss = 0.04413565\n",
      "Iteration 1390, loss = 0.04379343\n",
      "Iteration 1391, loss = 0.04397165\n",
      "Iteration 1392, loss = 0.04365409\n",
      "Iteration 1393, loss = 0.04351308\n",
      "Iteration 1394, loss = 0.04368264\n",
      "Iteration 1395, loss = 0.04335981\n",
      "Iteration 1396, loss = 0.04337714\n",
      "Iteration 1397, loss = 0.04326460\n",
      "Iteration 1398, loss = 0.04372634\n",
      "Iteration 1399, loss = 0.04323537\n",
      "Iteration 1400, loss = 0.04329710\n",
      "Iteration 1401, loss = 0.04296853\n",
      "Iteration 1402, loss = 0.04288400\n",
      "Iteration 1403, loss = 0.04300058\n",
      "Iteration 1404, loss = 0.04318107\n",
      "Iteration 1405, loss = 0.04291679\n",
      "Iteration 1406, loss = 0.04291672\n",
      "Iteration 1407, loss = 0.04269563\n",
      "Iteration 1408, loss = 0.04270461\n",
      "Iteration 1409, loss = 0.04268580\n",
      "Iteration 1410, loss = 0.04240888\n",
      "Iteration 1411, loss = 0.04226086\n",
      "Iteration 1412, loss = 0.04213713\n",
      "Iteration 1413, loss = 0.04210759\n",
      "Iteration 1414, loss = 0.04193676\n",
      "Iteration 1415, loss = 0.04192152\n",
      "Iteration 1416, loss = 0.04180361\n",
      "Iteration 1417, loss = 0.04256402\n",
      "Iteration 1418, loss = 0.04177634\n",
      "Iteration 1419, loss = 0.04167694\n",
      "Iteration 1420, loss = 0.04151163\n",
      "Iteration 1421, loss = 0.04128757\n",
      "Iteration 1422, loss = 0.04122706\n",
      "Iteration 1423, loss = 0.04088121\n",
      "Iteration 1424, loss = 0.04037560\n",
      "Iteration 1425, loss = 0.03984734\n",
      "Iteration 1426, loss = 0.03913290\n",
      "Iteration 1427, loss = 0.03900210\n",
      "Iteration 1428, loss = 0.03892349\n",
      "Iteration 1429, loss = 0.03909345\n",
      "Iteration 1430, loss = 0.04007067\n",
      "Iteration 1431, loss = 0.03861258\n",
      "Iteration 1432, loss = 0.03857209\n",
      "Iteration 1433, loss = 0.03870061\n",
      "Iteration 1434, loss = 0.03857042\n",
      "Iteration 1435, loss = 0.03870394\n",
      "Iteration 1436, loss = 0.03835070\n",
      "Iteration 1437, loss = 0.03819331\n",
      "Iteration 1438, loss = 0.03819038\n",
      "Iteration 1439, loss = 0.03836852\n",
      "Iteration 1440, loss = 0.03800664\n",
      "Iteration 1441, loss = 0.03811811\n",
      "Iteration 1442, loss = 0.03796071\n",
      "Iteration 1443, loss = 0.03823834\n",
      "Iteration 1444, loss = 0.03830592\n",
      "Iteration 1445, loss = 0.03789470\n",
      "Iteration 1446, loss = 0.03783333\n",
      "Iteration 1447, loss = 0.03767076\n",
      "Iteration 1448, loss = 0.03751418\n",
      "Iteration 1449, loss = 0.03814718\n",
      "Iteration 1450, loss = 0.03763153\n",
      "Iteration 1451, loss = 0.03772234\n",
      "Iteration 1452, loss = 0.03765476\n",
      "Iteration 1453, loss = 0.03756401\n",
      "Iteration 1454, loss = 0.03742240\n",
      "Iteration 1455, loss = 0.03747957\n",
      "Iteration 1456, loss = 0.03739132\n",
      "Iteration 1457, loss = 0.03714811\n",
      "Iteration 1458, loss = 0.03738291\n",
      "Iteration 1459, loss = 0.03728966\n",
      "Iteration 1460, loss = 0.03722109\n",
      "Iteration 1461, loss = 0.03739485\n",
      "Iteration 1462, loss = 0.03739920\n",
      "Iteration 1463, loss = 0.03732027\n",
      "Iteration 1464, loss = 0.03708998\n",
      "Iteration 1465, loss = 0.03699208\n",
      "Iteration 1466, loss = 0.03686614\n",
      "Iteration 1467, loss = 0.03686666\n",
      "Iteration 1468, loss = 0.03703669\n",
      "Iteration 1469, loss = 0.03715902\n",
      "Iteration 1470, loss = 0.03660278\n",
      "Iteration 1471, loss = 0.03698691\n",
      "Iteration 1472, loss = 0.03699276\n",
      "Iteration 1473, loss = 0.03719418\n",
      "Iteration 1474, loss = 0.03675219\n",
      "Iteration 1475, loss = 0.03651466\n",
      "Iteration 1476, loss = 0.03656470\n",
      "Iteration 1477, loss = 0.03646314\n",
      "Iteration 1478, loss = 0.03657315\n",
      "Iteration 1479, loss = 0.03640757\n",
      "Iteration 1480, loss = 0.03651003\n",
      "Iteration 1481, loss = 0.03616258\n",
      "Iteration 1482, loss = 0.03635684\n",
      "Iteration 1483, loss = 0.03639392\n",
      "Iteration 1484, loss = 0.03625322\n",
      "Iteration 1485, loss = 0.03610245\n",
      "Iteration 1486, loss = 0.03615235\n",
      "Iteration 1487, loss = 0.03600589\n",
      "Iteration 1488, loss = 0.03597458\n",
      "Iteration 1489, loss = 0.03607368\n",
      "Iteration 1490, loss = 0.03615040\n",
      "Iteration 1491, loss = 0.03590021\n",
      "Iteration 1492, loss = 0.03612123\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.08760774872853158\n"
     ]
    }
   ],
   "source": [
    "MLP = MLPRegressor(hidden_layer_sizes=200, batch_size=50, verbose=True, learning_rate='adaptive', learning_rate_init=1e-6, max_iter=5000)\n",
    "MLP.fit(X_train, y_train)\n",
    "prediction = MLP.predict(X_test)\n",
    "error = prediction - y_test\n",
    "MSE = np.sum(np.power(error, 2)) / error.shape[0]\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]0.04963080010557844\n"
     ]
    }
   ],
   "source": [
    "NSVR = NuSVR(verbose=True, kernel='poly', degree=10, C=10)\n",
    "NSVR.fit(X_train, y_train)\n",
    "prediction = NSVR.predict(X_test)\n",
    "error = prediction - y_test\n",
    "MSE = np.sum(np.power(error, 2)) / error.shape[0]\n",
    "print(MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10974840605154977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\ai-final\\lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "d:\\anaconda3\\envs\\ai-final\\lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "LSVR = LinearSVR(verbose=False, C=10)\n",
    "LSVR.fit(X_train, y_train)\n",
    "prediction = LSVR.predict(X_test)\n",
    "error = prediction - y_test\n",
    "MSE = np.sum(np.power(error, 2)) / error.shape[0]\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PCA' object has no attribute 'feature_names_in_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m PCAmodel \u001b[38;5;241m=\u001b[39m PCA()\n\u001b[0;32m      2\u001b[0m PCAmodel\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mPCAmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_names_in_\u001b[49m)\n\u001b[0;32m      4\u001b[0m PCAmodel\u001b[38;5;241m.\u001b[39mscore(X)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'feature_names_in_'"
     ]
    }
   ],
   "source": [
    "PCAmodel = PCA()\n",
    "PCAmodel.fit(X)\n",
    "PCAmodel.score(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
